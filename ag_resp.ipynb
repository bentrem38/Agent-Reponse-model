{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import Dataset\n",
    "import autopep8\n",
    "import os\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import evaluate\n",
    "from codebleu import calc_codebleu\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# 1. Install Required Libraries\n",
    "# ------------------------\n",
    "#!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "#!pip install transformers datasets evaluate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 800\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 2. Load Dataset \n",
    "# ------------------------------------------------------------------------\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"urvog/llama2_transcripts_healthcare_callcenter\")\n",
    "print(len(dataset[\"train\"][\"text\"]))\n",
    "train_set = dataset[\"train\"].select(range(0, 800))        \n",
    "validation_set = dataset[\"train\"].select(range(800, 900)) \n",
    "test_set = dataset[\"train\"].select(range(900, 1000))       \n",
    "\n",
    "\n",
    "split_dataset = DatasetDict({\n",
    "    \"train\": train_set,\n",
    "    \"validation\": validation_set,\n",
    "    \"test\": test_set\n",
    "})\n",
    "\n",
    "\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32101, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(dataset)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3. Load Pre-trained Model & Tokenizer\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "model_checkpoint = \"google/flan-t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.add_tokens([\"<MASK>\"]) #add <MASK> token to tokenizer\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 examples from validation\n",
      "Processed 100 examples from validation\n",
      "validation — Successfully masked: 95, Skipped: 5\n",
      "Processed 50 examples from test\n",
      "Processed 100 examples from test\n",
      "test — Successfully masked: 95, Skipped: 5\n",
      "Processed 50 examples from train\n",
      "Processed 100 examples from train\n",
      "Processed 150 examples from train\n",
      "Processed 200 examples from train\n",
      "Processed 250 examples from train\n",
      "Processed 300 examples from train\n",
      "Processed 350 examples from train\n",
      "Processed 400 examples from train\n",
      "Processed 450 examples from train\n",
      "Processed 500 examples from train\n",
      "Processed 550 examples from train\n",
      "Processed 600 examples from train\n",
      "Processed 650 examples from train\n",
      "Processed 700 examples from train\n",
      "Processed 750 examples from train\n",
      "Processed 800 examples from train\n",
      "train — Successfully masked: 766, Skipped: 34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3. Mask the 3rd-agent responses in the datasets\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def mask_dataset(dataset, datatype):\n",
    "    # Set max index based on split size\n",
    "    if datatype == \"test\" or datatype == \"validation\":\n",
    "        max = 99\n",
    "    elif datatype == \"train\":\n",
    "        max = 799\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown datatype: {datatype}\")\n",
    "        \n",
    "    processed_methods = []\n",
    "    processed_targets = []\n",
    "    i = 0\n",
    "\n",
    "    # Track success/failure\n",
    "    yes = 0\n",
    "    no = 0\n",
    "\n",
    "    # Loop through the dataset and apply masking\n",
    "    while i <= max:\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Processed {i + 1} examples from {datatype}\")\n",
    "\n",
    "        # Get original transcript\n",
    "        full_transcript = dataset[datatype]['text'][i]\n",
    "\n",
    "        # Flatten the transcript (remove newlines and extra whitespace)\n",
    "        flattened = \" \".join(full_transcript.split())\n",
    "\n",
    "        # Find all Agent responses using regex\n",
    "        agent_responses = re.findall(r\"Agent \\d+: (.*?)(?=Customer:|Agent \\d+:|$)\", flattened)\n",
    "\n",
    "        if len(agent_responses) >= 3:\n",
    "            target = agent_responses[2].strip()\n",
    "            masked = flattened.replace(target, \"<MASK>\", 2)\n",
    "\n",
    "            processed_methods.append(masked)\n",
    "            processed_targets.append(target)\n",
    "            yes += 1\n",
    "        else:\n",
    "            no += 1  # not enough agent responses to extract a 2nd one\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print(f\"{datatype} — Successfully masked: {yes}, Skipped: {no}\")\n",
    "    return {\n",
    "        \"processed_method\": processed_methods,\n",
    "        \"target_block\": processed_targets\n",
    "    }\n",
    "valid = mask_dataset(split_dataset, \"validation\")\n",
    "test = mask_dataset(split_dataset, \"test\")\n",
    "train = mask_dataset(split_dataset, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Classify the following call transcript: Agent 2: Thank you for calling HealthHarbor, my name is Agent 2. How may I assist you today? Customer: Hi Agent 2, I'm John Smith. I have some general inquiries about your hospital. Agent 2: Hello John, I'm here to help. What questions do you have? Customer: First, I would like to know what services HealthHarbor offers. Can you give me an overview? Agent 2: <MASK> Customer: That sounds great. Do you have any specific departments or specialties? Agent 2: Yes, we have various departments and specialties to cater to different medical needs. We have departments for cardiology, orthopedics, gastroenterology, neurology, pediatrics, and many others. Each department is staffed with highly trained medical professionals who are experts in their respective fields. Customer: That's impressive. How about the availability of doctors and appointment scheduling? Agent 2: We have a large team of doctors and specialists available at HealthHarbor. Our appointment scheduling system is designed to make it convenient for patients to book appointments. We strive to provide timely access to healthcare services, and our staff will assist you in finding a suitable appointment time. Customer: That's good to know. What about insurance coverage? Does HealthHarbor accept different insurance plans? Agent 2: Yes, HealthHarbor accepts a wide range of insurance plans. We work with major insurance providers to ensure that our services are accessible to as many patients as possible. However, it's always a good idea to check with your specific insurance provider to confirm coverage. Customer: Alright, I'll make sure to do that. Is HealthHarbor affiliated with any other healthcare organizations or networks? Agent 2: Yes, HealthHarbor is part of a larger healthcare network. We collaborate with other hospitals and medical centers to provide comprehensive care. This collaboration allows us to offer a broader range of services and access to specialized treatments if needed. Customer: That's reassuring. Lastly, I'd like to know if HealthHarbor has any patient support programs or resources available. Agent 2: Absolutely, John. We understand the importance of patient support and offer various programs to assist our patients. We have support groups, counseling services, educational resources, and financial assistance programs for those who qualify. Our goal is to provide comprehensive care and support to our patients. Customer: That's great to hear. Thank you for answering all my questions, Agent 2. You've been very helpful. Agent 2: You're welcome, John. I'm glad I could assist you. If you have any more questions in the future or if there's anything else I can help you with, please don't hesitate to reach out. Have a great day! Customer: Thank you, Agent 2. I appreciate your assistance. Have a great day too! Goodbye. Agent 2: Goodbye, John. Take care! [/INST] General Inquiries </s>\n",
      "Certainly, John. HealthHarbor is a comprehensive healthcare facility that offers a wide range of medical services. We provide primary care, specialized treatments, surgeries, diagnostic tests, emergency care, and much more.\n",
      "Dataset({\n",
      "    features: ['processed_method', 'target_block'],\n",
      "    num_rows: 766\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#print(train[\"processed_method\"][1])\n",
    "#print(train[\"target_block\"][1])\n",
    "#print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 766/766 [00:00<00:00, 2030.59 examples/s]\n",
      "Map: 100%|██████████| 95/95 [00:00<00:00, 2064.76 examples/s]\n",
      "Map: 100%|██████████| 95/95 [00:00<00:00, 2110.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# 4. We prepare now the fine-tuning dataset using the tokenizer we preloaded\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "\n",
    "def preprocess_function(dataset):\n",
    "    inputs = dataset[\"processed_method\"]\n",
    "    targets = dataset[\"target_block\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "train = train.map(preprocess_function, batched=True)\n",
    "valid = valid.map(preprocess_function, batched = True)\n",
    "test = test.map(preprocess_function, batched = True)\n",
    "#print(valid)\n",
    "#print(train)\n",
    "#print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bentr\\AppData\\Local\\Temp\\ipykernel_21292\\1982947724.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# 5. Define Training Arguments and Trainer\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\".google/flan-t5-base\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=valid,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1915' max='3830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1915/3830 09:33 < 09:34, 3.33 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.094700</td>\n",
       "      <td>0.111950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.110143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.109998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>0.114833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.115739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1915, training_loss=0.07693713601513258, metrics={'train_runtime': 574.0438, 'train_samples_per_second': 13.344, 'train_steps_per_second': 6.672, 'total_flos': 1311188342538240.0, 'train_loss': 0.07693713601513258, 'epoch': 5.0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------\n",
    "# 6. Train the Model\n",
    "# ------------------------\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.google/flan-t5-base\\\\tokenizer_config.json',\n",
       " '.google/flan-t5-base\\\\special_tokens_map.json',\n",
       " '.google/flan-t5-base\\\\tokenizer.json')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \".google/flan-t5-base\"\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(save_path)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_path = \".google/flan-t5-base\"\n",
    "# Load the saved model\n",
    "model = T5ForConditionalGeneration.from_pretrained(save_path)\n",
    "\n",
    "# Load the saved tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course, Sarah. HealthHarbor is a hospital that offers a wide range of medical services. We have specialized doctors, advanced diagnostic tools, and state-of-the-art facilities. If you're experiencing serious symptoms or if it's an emergency, it's best to visit our hospital. However, for general check-ups or less urgent issues, you can consider visiting a clinic.\n",
      "<pad> Customer: Hello. How can I help you?</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------\n",
    "# 7. Test Code Translation\n",
    "# ------------------------\n",
    "model2 = model.to('cuda')\n",
    "input_code = test[\"processed_method\"][2]\n",
    "print(test[\"target_block\"][2])\n",
    "inputs = tokenizer(input_code, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "outputs = model2.generate(**inputs.to('cuda'), max_length=256)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "model2.eval()\n",
    "\n",
    "all_inputs = test[\"processed_method\"]\n",
    "batch_size = 8  \n",
    "decoded_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:08<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 8. Run the model generation in batches in order to run code without memory errors\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "for i in tqdm(range(0, len(all_inputs), batch_size)):\n",
    "    batch = all_inputs[i:i+batch_size]\n",
    "\n",
    "    # Tokenize batch\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model2.generate(**inputs, max_length=256)\n",
    "\n",
    "    # Decode each output\n",
    "    decoded_batch = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    decoded_outputs.extend(decoded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you, Mr. Smith. How can I assist you today?\n",
      "Prediction: Customer: Hello\n",
      "Thank you for providing that information, Sarah. Now, please let me know what specific medication or prescription inquiries you have.\n",
      "Prediction: Customer: Yes, that's right.\n",
      "Of course, Sarah. HealthHarbor is a hospital that offers a wide range of medical services. We have specialized doctors, advanced diagnostic tools, and state-of-the-art facilities. If you're experiencing serious symptoms or if it's an emergency, it's best to visit our hospital. However, for general check-ups or less urgent issues, you can consider visiting a clinic.\n",
      "Prediction: Customer: Hello. How can I help you?\n",
      "Thank you, Ms. Johnson. How can I assist you with your specialist appointment?\n",
      "Prediction: Customer: Thank you for calling HealthHarbor.\n",
      "Thank you, Emily. How can I assist you specifically with your medication refills and prescription inquiries?\n",
      "Prediction: Customer\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(test[\"target_block\"][i])\n",
    "    print(f\"Prediction: {decoded_outputs[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      "rouge1: 0.0663\n",
      "rouge2: 0.0168\n",
      "rougeL: 0.0593\n",
      "rougeLsum: 0.0591\n",
      "Overall ROUGE Score:  0.0503776024659935\n"
     ]
    }
   ],
   "source": [
    "rouge = load(\"rouge\")\n",
    "predictions = decoded_outputs\n",
    "references = test[\"target_block\"]\n",
    "# Calculate the ROUGE scores\n",
    "rouge_scores = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "# Print separate ROUGE scores\n",
    "print(\"ROUGE Scores:\")\n",
    "for metric, score in rouge_scores.items():\n",
    "    print(f\"{metric}: {round(score, 4)}\")\n",
    "print(\"Overall ROUGE Score: \", sum(rouge_scores.values()) / len(rouge_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
